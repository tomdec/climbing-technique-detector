{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634b9adb",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19670289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.impute import SimpleImputer\n",
    "from numpy import nan, arange, array\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from numpy import ndarray, mean, float32\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from typing import Tuple\n",
    "\n",
    "from src.labels import iterate_valid_labels, find_valid_segments, get_labels_from_video\n",
    "from src.common.helpers import read_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d983dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmark_df_path(video_path: str) -> str:\n",
    "    return video_path.replace(\"/videos/\", \"/df/videos/\").replace(\".mp4\", \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd058aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"data/videos/Route9Climb1.mp4\"\n",
    "label_path = get_labels_from_video(video_path)\n",
    "hpe_path = get_landmark_df_path(video_path)\n",
    "\n",
    "valids = find_valid_segments(label_path)\n",
    "df = read_dataframe(hpe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_segments = []\n",
    "for valid_segment_idx in valids:\n",
    "    valid_segment_slice = slice(valid_segment_idx[0], valid_segment_idx[1])\n",
    "    valid_segments.append(df[valid_segment_slice])\n",
    "\n",
    "print(f\"Valid segments: {len(valid_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783127f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: replace with iteration over all valid segments\n",
    "temp = valid_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fadb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_features(features: DataFrame) -> DataFrame:\n",
    "\toutput = features.copy()\n",
    "\tframe_num = output.pop('frame_num')\n",
    "\timp = SimpleImputer(missing_values=nan, strategy='constant', fill_value=0, \n",
    "\t\tkeep_empty_features=True)\n",
    "\toutput = DataFrame(imp.fit_transform(output), columns=output.keys())\n",
    "\toutput['frame_num'] = frame_num\n",
    "\treturn output\n",
    "\n",
    "def normalize_features(train_df: DataFrame, val_df: DataFrame, test_df: DataFrame):\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: reuse from src.hpe_dnn.helpers\n",
    "def binarize_labels(labels: Series) -> ndarray:\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(list(iterate_valid_labels()))\n",
    "    return encoder.transform(labels)\n",
    "\n",
    "def unbinarize_labels(logits: Series):\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(list(iterate_valid_labels()))\n",
    "    return encoder.inverse_transform(logits)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "\tdef __init__(self, input_width: int, label_width: int, shift: int, valid_segment: DataFrame):\n",
    "\t\t# Store the raw data.\n",
    "\t\tdf = valid_segment.copy()\n",
    "\n",
    "\t\t# Transform labels to model ouputs\n",
    "\t\tlabels_str = df.pop('label')\n",
    "\t\tlabels_bin = binarize_labels(labels_str)\n",
    "\t\tself.labels = DataFrame(data=labels_bin, columns=list(iterate_valid_labels()))\n",
    "\t\t\n",
    "\t\t# Remaining columns are the input features\n",
    "\t\tself.features = impute_features(df)\n",
    "\n",
    "\t\t# Work out the label column indices.\n",
    "\t\tself.column_indices = {name: i for i, name in enumerate(self.features.columns)}\n",
    "\n",
    "\t\t# Work out the window parameters.\n",
    "\t\tself.input_width = input_width\n",
    "\t\tself.label_width = label_width\n",
    "\t\tself.shift = shift\n",
    "\n",
    "\t\tself.total_window_size = input_width + shift\n",
    "\n",
    "\t\tself.input_slice = slice(0, input_width)\n",
    "\t\tself.input_indices = arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "\t\tself.label_start = self.total_window_size - self.label_width\n",
    "\t\tself.labels_slice = slice(self.label_start, None)\n",
    "\t\tself.label_indices = arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "\tdef split_window(self, feature_batch, label_batch) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "\t\t\"\"\"Split the features and labels, of length self.total_windows_size into an (input, output)\n",
    "\t\tdata pair.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tfeatures: input features over current window.\n",
    "\t\t\tlabels: output labels over current window.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tTuple[tf.data.Datasets, tf.data.Datasets]: (input, output) pair.\n",
    "\t\t\"\"\"\n",
    "\t\tinputs = feature_batch[:, self.input_slice, :]\n",
    "\t\toutput = label_batch[:, self.labels_slice, :]\n",
    "\t\t\n",
    "\t\t# Slicing doesn't preserve static shape information, so set the shapes\n",
    "\t\t# manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "\t\tinputs.set_shape([None, self.input_width, None])\n",
    "\t\toutput.set_shape([None, self.label_width, None])\n",
    "\n",
    "\t\treturn inputs, output\n",
    "\n",
    "\tdef get_example(self):\n",
    "\t\t# Stack three slices, the length of the total window.\n",
    "\t\texample_features = tf.stack([array(self.features[:self.total_window_size]),\n",
    "\t\t\tarray(self.features[100:100+self.total_window_size]),\n",
    "\t\t\tarray(self.features[200:200+self.total_window_size])])\n",
    "\t\t\n",
    "\t\texample_labels = tf.stack([array(self.labels[:self.total_window_size]),\n",
    "\t\t\tarray(self.labels[100:100+self.total_window_size]),\n",
    "\t\t\tarray(self.labels[200:200+self.total_window_size])])\n",
    "\n",
    "\t\texample_inputs, example_ouputs = self.split_window(example_features, example_labels)\n",
    "\n",
    "\t\tprint('All shapes are: (batch, time, features)')\n",
    "\t\tprint(f'Window shape: {example_features.shape}')\n",
    "\t\tprint(f'Inputs shape: {example_inputs.shape}')\n",
    "\t\tprint(f'Labels shape: {example_ouputs.shape}')\n",
    "\n",
    "\t\treturn example_inputs, example_ouputs\n",
    "\n",
    "\tdef plot(self, model=None, plot_col='NOSE_x', max_subplots=3):\n",
    "\t\tinputs, labels = self.get_example()\n",
    "\t\tplt.figure(figsize=(12, 8))\n",
    "\t\tplot_col_index = self.column_indices[plot_col]\n",
    "\t\tframe_num_index = self.column_indices['frame_num']\n",
    "\t\tmax_n = min(max_subplots, len(inputs))\n",
    "\t\tfor n in range(max_n):\n",
    "\t\t\tplt.subplot(max_n, 1, n+1)\n",
    "\t\t\tplt.ylabel(f'{plot_col} [act]')\n",
    "\t\t\tfeature_values = inputs[n, :, plot_col_index]\n",
    "\t\t\tx_axis = inputs[n, :, frame_num_index]\n",
    "\t\t\t\n",
    "\t\t\tplt.xlim((x_axis[0], x_axis[-1]+1))\n",
    "\t\t\tplt.plot(x_axis, feature_values,\n",
    "\t\t\t\tlabel='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "\t\t\tlabel_col_index = plot_col_index\n",
    "\t\t\tif label_col_index is None:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\tlabel_name = unbinarize_labels(array(labels[n, :, :]))\n",
    "\t\t\tlabel_x_position = x_axis[-1] + 0.2\n",
    "\t\t\tplt.text(label_x_position, mean(feature_values), label_name, c='#2ca02c', \n",
    "\t\t\t\tlabel=\"Label\")\n",
    "\t\t\t\n",
    "\t\t\t# plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "\t\t\t# \tedgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "\t\t\t# if model is not None:\n",
    "\t\t\t# \tpredictions = model(inputs)\n",
    "\t\t\t# \tplt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "\t\t\t# \t\tmarker='X', edgecolors='k', label='Predictions',\n",
    "\t\t\t# \t\tc='#ff7f0e', s=64)\n",
    "\n",
    "\t\t\tif n == 0:\n",
    "\t\t\t\tplt.legend()\n",
    "\n",
    "\t\tplt.xlabel('Frame number')\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn '\\n'.join([\n",
    "\t\t\tf'Total window size: {self.total_window_size}',\n",
    "\t\t\tf'Input indices: {self.input_indices}',\n",
    "\t\t\tf'Label indices: {self.label_indices}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = WindowGenerator(input_width=5, label_width=1, shift=1, valid_segment=temp)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = w1.make_dataset(w1.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing, do this in the training pipeline, determine values only from training data\n",
    "train_mean = temp.mean()\n",
    "train_std = temp.std()\n",
    "\n",
    "temp = (temp - train_mean) / train_std\n",
    "#temp_val = ...\n",
    "#temp_test = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e763d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "features_melted = temp.melt(var_name=\"Column\", value_name=\"Raw\")\n",
    "ax = sns.violinplot(x=\"Column\", y=\"Raw\", data=features_melted)\n",
    "_ = ax.set_xticklabels(temp.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f143ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from src.common.model import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(ClassificationModel):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee21c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
