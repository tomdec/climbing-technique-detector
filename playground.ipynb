{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe.mp.performance import EstimationCollector\n",
    "\n",
    "collector = EstimationCollector()\n",
    "df = collector.collect(name=\"estimations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe.mp.performance import read_estimations\n",
    "\n",
    "df = read_estimations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53de21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 40.2ms\n",
      "Speed: 34.8ms preprocess, 40.2ms inference, 925.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 40.4ms\n",
      "Speed: 3.9ms preprocess, 40.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 16.1ms\n",
      "Speed: 2.8ms preprocess, 16.1ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 (no detections), 16.9ms\n",
      "Speed: 2.6ms preprocess, 16.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 16.3ms\n",
      "Speed: 2.7ms preprocess, 16.3ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 18.0ms\n",
      "Speed: 2.6ms preprocess, 18.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 15.9ms\n",
      "Speed: 2.6ms preprocess, 15.9ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 20.7ms\n",
      "Speed: 2.6ms preprocess, 20.7ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 15.9ms\n",
      "Speed: 2.8ms preprocess, 15.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 16.9ms\n",
      "Speed: 2.7ms preprocess, 16.9ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 17.1ms\n",
      "Speed: 2.8ms preprocess, 17.1ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 15.4ms\n",
      "Speed: 2.8ms preprocess, 15.4ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 17.2ms\n",
      "Speed: 2.7ms preprocess, 17.2ms inference, 2.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 17.2ms\n",
      "Speed: 2.8ms preprocess, 17.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 17.0ms\n",
      "Speed: 2.6ms preprocess, 17.0ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 16.6ms\n",
      "Speed: 3.1ms preprocess, 16.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 15.5ms\n",
      "Speed: 2.8ms preprocess, 15.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 15.9ms\n",
      "Speed: 2.9ms preprocess, 15.9ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 15.6ms\n",
      "Speed: 2.6ms preprocess, 15.6ms inference, 2.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 17.7ms\n",
      "Speed: 3.7ms preprocess, 17.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 16.4ms\n",
      "Speed: 3.0ms preprocess, 16.4ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 14.4ms\n",
      "Speed: 2.8ms preprocess, 14.4ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 15.6ms\n",
      "Speed: 3.0ms preprocess, 15.6ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 15.5ms\n",
      "Speed: 3.0ms preprocess, 15.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 14.3ms\n",
      "Speed: 2.9ms preprocess, 14.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 16.1ms\n",
      "Speed: 3.1ms preprocess, 16.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 16.4ms\n",
      "Speed: 3.0ms preprocess, 16.4ms inference, 4.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 14.5ms\n",
      "Speed: 3.1ms preprocess, 14.5ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 15.3ms\n",
      "Speed: 2.7ms preprocess, 15.3ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 17.1ms\n",
      "Speed: 3.1ms preprocess, 17.1ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 14.8ms\n",
      "Speed: 2.7ms preprocess, 14.8ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 16.0ms\n",
      "Speed: 2.9ms preprocess, 16.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 15.4ms\n",
      "Speed: 2.9ms preprocess, 15.4ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 15.1ms\n",
      "Speed: 3.1ms preprocess, 15.1ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 16.3ms\n",
      "Speed: 3.4ms preprocess, 16.3ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    }
   ],
   "source": [
    "from src.hpe.yolo.performance import EstimationCollector as YoloEstimationCollector\n",
    "\n",
    "collector = YoloEstimationCollector()\n",
    "df = collector.collect(name=\"estimations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f327ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe.yolo.performance import read_distances as read_yolo_distances\n",
    "from src.hpe.mp.performance import read_distances as read_mp_distances\n",
    "from src.hpe.common.plot import plot_distances_boxplot\n",
    "\n",
    "mp_dist = read_mp_distances(dataset_name=\"distances\")\n",
    "mp_bgr_dist = read_mp_distances(dataset_name=\"distances_bgr\")\n",
    "\n",
    "yolo_dist = read_yolo_distances(dataset_name=\"distances\")\n",
    "yolo_bgr_dist = read_yolo_distances(dataset_name=\"distances_bgr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aab6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distances_boxplot(\n",
    "    None,\n",
    "    (\"MediaPipe\", mp_dist),\n",
    "    (\"MediaPipe on bgr\", mp_bgr_dist),\n",
    "    (\"Yolov11\", yolo_dist),\n",
    "    (\"Yolov11 on bgr\", yolo_bgr_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distances_boxplot(\n",
    "    (0, 10),\n",
    "    (\"MediaPipe\", mp_dist),\n",
    "    (\"MediaPipe on bgr\", mp_bgr_dist),\n",
    "    (\"Yolov11\", yolo_dist),\n",
    "    (\"Yolov11 on bgr\", yolo_bgr_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.kfold import HpeDnnFoldCrossValidation\n",
    "from src.sota.kfold import SOTAFoldCrossValidation\n",
    "from src.common.plot import box_plot_accuracies\n",
    "\n",
    "box_plot_accuracies(\n",
    "    HpeDnnFoldCrossValidation.evaluation_instance(\"arch1-mp-kf\"),\n",
    "    HpeDnnFoldCrossValidation.evaluation_instance(\"arch1-mp-unbalanced-kf\"),\n",
    "    HpeDnnFoldCrossValidation.evaluation_instance(\"arch1-mp-dr0.3-kf\"),\n",
    "    HpeDnnFoldCrossValidation.evaluation_instance(\"arch1-yolo-kf\"),\n",
    "    # HpeDnnFoldCrossValidation.evaluation_instance(\"arch2-kf\"), \n",
    "    # HpeDnnFoldCrossValidation.evaluation_instance(\"arch3-kf\"), \n",
    "    # HpeDnnFoldCrossValidation.evaluation_instance(\"arch4-kf\"), \n",
    "    # HpeDnnFoldCrossValidation.evaluation_instance(\"arch5-kf\"),\n",
    "    SOTAFoldCrossValidation.evaluation_instance(\"yolo11n-kf\", \"yolov11n-cls\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
