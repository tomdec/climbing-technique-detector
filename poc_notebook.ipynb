{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sampling.images import plot_frame_count_distributions\n",
    "\n",
    "samples_root_dir = \"data/samples\"\n",
    "\n",
    "plot_frame_count_distributions(samples_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sota model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sota.model import SOTA\n",
    "\n",
    "sota = SOTA(\"data\", \"yolo11m-cls\")\n",
    "sota.initialize_model()\n",
    "sota.execute_train_runs(model=\"yolo11n-cls\", runs=2, epochs=5, balanced=False)\n",
    "#sota.train_model(optimizer=\"AdamW\", lr0=0.0005)\n",
    "\n",
    "#metrics = model.val(data=\"data/img/techniques/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sota.model import SOTA\n",
    "\n",
    "sota = SOTA(\"data\", \"yolo11n-cls\")\n",
    "sota.initialize_model()\n",
    "sota.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sota.model import SOTA\n",
    "\n",
    "sota = SOTA(\"data\", \"yolo11m-balance-50-155\", dataset_name=\"techniques_balanced\")\n",
    "sota.initialize_model(\"yolo11m-cls\")\n",
    "sota.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sota.model import SOTA\n",
    "\n",
    "sota = SOTA(\"data\", \"yolo11n-balance-50-155\", dataset_name=\"techniques_balanced\")\n",
    "sota.initialize_model(\"yolo11n-cls\")\n",
    "#sota.train_model()\n",
    "sota.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Projects/climbing-technique-detector/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m This integration is tested and supported for ultralytics v8.0.238 and below.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m             Please report any issues to https://github.com/wandb/wandb/issues with the tag `yolov8`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting run #0\n",
      "loading a fresh model 'yolo11n-cls'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtom-decoutere\u001b[0m (\u001b[33mtom-decoutere-open-universiteit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>data/wandb/run-20250716_144230-yolo11n-cls-full-balanced</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced' target=\"_blank\">yolo11n-cls-full-balanced</a></strong> to <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.167 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.78 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA TITAN RTX, 24576MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolo11n-cls.pt, data=data/img/techniques, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=data/runs/sota/yolo11n-cls-full-balanced, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=data/runs/sota/yolo11n-cls-full-balanced/train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/train... found 3850 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/val... found 804 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/test... found 836 images in 7 classes ‚úÖ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752669765.719828   24747 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752669766.023803   24747 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    339207  ultralytics.nn.modules.head.Classify         [256, 7]                      \n",
      "YOLO11n-cls summary: 86 layers, 1,540,071 parameters, 1,540,071 gradients, 3.3 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir data/runs/sota/yolo11n-cls-full-balanced/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/c/Projects/climbing-technique-detector/data/img/techniques/train... 3850 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3850/3850 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BACK_FLAG': 230, 'CROSS_MIDLINE': 155, 'DROP_KNEE': 306, 'FOOT_SWAP': 100, 'INSIDE_FLAG': 240, 'NONE': 2488, 'OUTSIDE_FLAG': 331}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Projects/climbing-technique-detector/data/img/techniques/val... 804 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 804/804 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BACK_FLAG': 43, 'CROSS_MIDLINE': 37, 'DROP_KNEE': 57, 'FOOT_SWAP': 19, 'INSIDE_FLAG': 51, 'NONE': 535, 'OUTSIDE_FLAG': 62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mdata/runs/sota/yolo11n-cls-full-balanced/train\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      1.64G      1.456         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [01:02<00:00,  3.87it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.372      0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.78 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA TITAN RTX, 24576MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 2,807,024 parameters, 0 gradients, 4.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 176.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      1.66G     0.9834         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:54<00:00,  4.38it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:07<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.43      0.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 166.90it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 184.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      1.66G     0.7433         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:53<00:00,  4.54it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.474      0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.43it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 176.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      1.66G     0.5667         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:52<00:00,  4.62it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:10<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.7      0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 145.72it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 182.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      1.68G     0.4423         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:50<00:00,  4.75it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:09<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.703      0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 175.32it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 133.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.095 hours.\n",
      "Optimizer stripped from data/runs/sota/yolo11n-cls-full-balanced/train/weights/last.pt, 3.2MB\n",
      "Optimizer stripped from data/runs/sota/yolo11n-cls-full-balanced/train/weights/best.pt, 3.2MB\n",
      "\n",
      "Validating data/runs/sota/yolo11n-cls-full-balanced/train/weights/best.pt...\n",
      "Ultralytics 8.3.78 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA TITAN RTX, 24576MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 1,534,991 parameters, 0 gradients, 3.2 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/train... found 3850 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/val... found 804 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/test... found 836 images in 7 classes ‚úÖ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:07<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.708      0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 110.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms preprocess, 0.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mdata/runs/sota/yolo11n-cls-full-balanced/train\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>‚ñÉ‚ñá‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>lr/pg1</td><td>‚ñÉ‚ñá‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>lr/pg2</td><td>‚ñÉ‚ñá‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>metrics/accuracy_top1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñà</td></tr><tr><td>metrics/accuracy_top5</td><td>‚ñÅ‚ñÖ‚ñà‚ñà‚ñà</td></tr><tr><td>model/GFLOPs</td><td>‚ñÅ</td></tr><tr><td>model/parameters</td><td>‚ñÅ</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>val/loss</td><td>‚ñà‚ñá‚ñÜ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00019</td></tr><tr><td>lr/pg1</td><td>0.00019</td></tr><tr><td>lr/pg2</td><td>0.00019</td></tr><tr><td>metrics/accuracy_top1</td><td>0.70771</td></tr><tr><td>metrics/accuracy_top5</td><td>0.99129</td></tr><tr><td>model/GFLOPs</td><td>3.259</td></tr><tr><td>model/parameters</td><td>1540071</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>1.011</td></tr><tr><td>train/loss</td><td>0.44232</td></tr><tr><td>val/loss</td><td>0.94556</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolo11n-cls-full-balanced</strong> at: <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced</a><br> View project at: <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique</a><br>Synced 5 W&B file(s), 14 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>data/wandb/run-20250716_144230-yolo11n-cls-full-balanced/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
      "\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7f569b6a3740>\n",
      "curves: []\n",
      "curves_results: []\n",
      "fitness: 0.8495025038719177\n",
      "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
      "results_dict: {'metrics/accuracy_top1': 0.7077114582061768, 'metrics/accuracy_top5': 0.9912935495376587, 'fitness': 0.8495025038719177}\n",
      "save_dir: PosixPath('data/runs/sota/yolo11n-cls-full-balanced/train')\n",
      "speed: {'preprocess': 0.44651210692107957, 'inference': 0.624020262446141, 'loss': 0.00026386070244167165, 'postprocess': 0.004306997498902794}\n",
      "task: 'classify'\n",
      "top1: 0.7077114582061768\n",
      "top5: 0.9912935495376587\n",
      "starting run #1\n",
      "loading the model 'yolo11n-cls-full-balanced' with the weights at 'data/runs/sota/yolo11n-cls-full-balanced/train/weights/best.pt'\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>data/wandb/run-20250716_145015-yolo11n-cls-full-balanced</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced' target=\"_blank\">yolo11n-cls-full-balanced</a></strong> to <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.167 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.78 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA TITAN RTX, 24576MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=data/runs/sota/yolo11n-cls-full-balanced/train/weights/best.pt, data=data/img/techniques, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=data/runs/sota/yolo11n-cls-full-balanced, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=data/runs/sota/yolo11n-cls-full-balanced/train2\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/train... found 3850 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/val... found 804 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/test... found 836 images in 7 classes ‚úÖ \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    339207  ultralytics.nn.modules.head.Classify         [256, 7]                      \n",
      "YOLO11n-cls summary: 86 layers, 1,540,071 parameters, 1,540,071 gradients, 3.3 GFLOPs\n",
      "Transferred 236/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir data/runs/sota/yolo11n-cls-full-balanced/train2', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/c/Projects/climbing-technique-detector/data/img/techniques/train... 3850 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3850/3850 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BACK_FLAG': 230, 'CROSS_MIDLINE': 155, 'DROP_KNEE': 306, 'FOOT_SWAP': 100, 'INSIDE_FLAG': 240, 'NONE': 2488, 'OUTSIDE_FLAG': 331}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Projects/climbing-technique-detector/data/img/techniques/val... 804 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 804/804 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BACK_FLAG': 43, 'CROSS_MIDLINE': 37, 'DROP_KNEE': 57, 'FOOT_SWAP': 19, 'INSIDE_FLAG': 51, 'NONE': 535, 'OUTSIDE_FLAG': 62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mdata/runs/sota/yolo11n-cls-full-balanced/train2\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      1.67G     0.3741         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:58<00:00,  4.14it/s]\n",
      "               classes   top1_acc   top5_acc:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:05<00:00,  2.91it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.667      0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.78 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA TITAN RTX, 24576MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 1,534,991 parameters, 0 gradients, 3.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.17it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 172.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      1.68G     0.3875         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:52<00:00,  4.63it/s]\n",
      "               classes   top1_acc   top5_acc:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 33.71it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:09<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.716      0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 117.37it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 150.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      1.66G     0.3268         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:54<00:00,  4.40it/s]\n",
      "               classes   top1_acc   top5_acc:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:10<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.685      0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 90.73it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 129.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      1.67G     0.2725         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:55<00:00,  4.36it/s]\n",
      "               classes   top1_acc   top5_acc:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:05<00:01,  2.90it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.762      0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 125.60it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 142.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      1.65G     0.2332         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [00:54<00:00,  4.45it/s]\n",
      "               classes   top1_acc   top5_acc:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 31.47it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:07<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.751      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 186.04it/s]\n",
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 150.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.095 hours.\n",
      "Optimizer stripped from data/runs/sota/yolo11n-cls-full-balanced/train2/weights/last.pt, 3.2MB\n",
      "Optimizer stripped from data/runs/sota/yolo11n-cls-full-balanced/train2/weights/best.pt, 3.2MB\n",
      "\n",
      "Validating data/runs/sota/yolo11n-cls-full-balanced/train2/weights/best.pt...\n",
      "Ultralytics 8.3.78 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA TITAN RTX, 24576MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 1,534,991 parameters, 0 gradients, 3.2 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/train... found 3850 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/val... found 804 images in 7 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /mnt/c/Projects/climbing-technique-detector/data/img/techniques/test... found 836 images in 7 classes ‚úÖ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:07<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.78       0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Visualizations for batch-1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 169.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mdata/runs/sota/yolo11n-cls-full-balanced/train2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00019</td></tr><tr><td>lr/pg1</td><td>0.00019</td></tr><tr><td>lr/pg2</td><td>0.00019</td></tr><tr><td>metrics/accuracy_top1</td><td>0.70771</td></tr><tr><td>metrics/accuracy_top5</td><td>0.99129</td></tr><tr><td>model/GFLOPs</td><td>3.259</td></tr><tr><td>model/parameters</td><td>1540071</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>1.011</td></tr><tr><td>train/loss</td><td>0.44232</td></tr><tr><td>val/loss</td><td>0.94556</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolo11n-cls-full-balanced</strong> at: <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique/runs/yolo11n-cls-full-balanced</a><br> View project at: <a href='https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique' target=\"_blank\">https://wandb.ai/tom-decoutere-open-universiteit/detect-climbing-technique</a><br>Synced 5 W&B file(s), 14 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>data/wandb/run-20250716_145015-yolo11n-cls-full-balanced/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tried to log to step 5 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
      "\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7f56b4739370>\n",
      "curves: []\n",
      "curves_results: []\n",
      "fitness: 0.8849502503871918\n",
      "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
      "results_dict: {'metrics/accuracy_top1': 0.7798507213592529, 'metrics/accuracy_top5': 0.9900497794151306, 'fitness': 0.8849502503871918}\n",
      "save_dir: PosixPath('data/runs/sota/yolo11n-cls-full-balanced/train2')\n",
      "speed: {'preprocess': 0.42342952485287516, 'inference': 0.7378381019837104, 'loss': 0.0035888818396714087, 'postprocess': 0.0007910647089112392}\n",
      "task: 'classify'\n",
      "top1: 0.7798507213592529\n",
      "top5: 0.9900497794151306\n"
     ]
    }
   ],
   "source": [
    "from src.sota.model import SOTA\n",
    "\n",
    "sota = SOTA(\"data\", \"yolo11n-cls-full-balanced\")\n",
    "sota.execute_train_runs(model=\"yolo11n-cls\", runs=2, epochs=5, balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sota.model import SOTA\n",
    "\n",
    "sota = SOTA(\"data\", \"yolo11m-cls-full-balanced\")\n",
    "sota.execute_train_runs(model=\"yolo11m-cls\", runs=2, epochs=10, balanced=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPE DNN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch1\")\n",
    "hpednn.initialize_model()\n",
    "hpednn.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn\n",
    "\n",
    "hpednn_aug = HpeDnn(\"data\", \"arch1_augmented\")\n",
    "hpednn_aug.initialize_model()\n",
    "hpednn_aug.train_model(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "from src.hpe_dnn.model import HpeDnn\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch1_balanced\", \"techniques_balanced\")\n",
    "hpednn.initialize_model()\n",
    "hpednn.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sota/yolo11n-cls-full-balanced'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import join\n",
    "run_dir = join(\"data\", \"sota\", \"yolo11n-cls-full-balanced\")\n",
    "run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch1_balanced_augmented\", \"techniques_balanced\")\n",
    "hpednn.initialize_model()\n",
    "hpednn.train_model(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn, DnnArch\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch2_balanced\", \"techniques_balanced\")\n",
    "hpednn.initialize_model(DnnArch.ARCH2)\n",
    "hpednn.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn, DnnArch\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch3_balanced\", \"techniques_balanced\")\n",
    "hpednn.initialize_model(DnnArch.ARCH3)\n",
    "hpednn.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch1_full_balanced\")\n",
    "hpednn.execute_train_runs(runs=2, epochs=10, augment=True, balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn, DnnArch\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch1_balanced_not_norm\", \"techniques_balanced\")\n",
    "hpednn.initialize_model(DnnArch.ARCH1, normalize=False)\n",
    "hpednn.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import HpeDnn, DnnArch\n",
    "\n",
    "hpednn = HpeDnn(\"data\", \"arch1_balanced_dr_0.3\", \"techniques_balanced\")\n",
    "hpednn.initialize_model(DnnArch.ARCH1, dropout_rate=0.3)\n",
    "hpednn.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir data/runs/hpe_dnn/arch1_balanced/train1/logs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hpe_dnn.model import read_data\n",
    "\n",
    "df_path = \"data/df/techniques/train.pkl\"\n",
    "train = read_data(df_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
